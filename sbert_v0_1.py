# -*- coding: utf-8 -*-
"""SBERT_v0_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-RFpthe-wlIPC8I3pcjiEc5KimtGC57Q
"""

import time # time 라이브러리 import
start = time.time() # 시작

!pip install ko-sentence-transformers

from sentence_transformers import SentenceTransformer, models
from ko_sentence_transformers.models import KoBertTransformer
# word_embedding_model = KoBertTransformer("monologg/kobert", max_seq_length=75)
# pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), pooling_mode='mean')
# model = SentenceTransformer(modules=[word_embedding_model, pooling_model])

# f01 = open("/content/drive/MyDrive/Colab Notebooks/SentenceBERT/Data/rfp03-1.txt","r")
f01 = open("/content/drive/MyDrive/Colab Notebooks/SentenceBERT/Data/국토부.txt","r")
rfp01 = f01.readlines()
rfp01 = list(map(lambda s: s.strip(), rfp01)) # rfp01 = [x for x in rfp01]
print(len(rfp01))
rfp01

# f02 = open("/content/drive/MyDrive/Colab Notebooks/SentenceBERT/Data/rfp04-1.txt","r")
f02 = open("/content/drive/MyDrive/Colab Notebooks/SentenceBERT/Data/국토부.txt","r")
rfp02 = f02.readlines()
rfp02 = list(map(lambda s: s.strip(), rfp02)) # rfp02 = [x for x in rfp02]
print(len(rfp02))
rfp02

from sentence_transformers import SentenceTransformer, util
import numpy as np

f03 = open("/content/drive/MyDrive/Colab Notebooks/SentenceBERT/graph01.txt", 'w')
f04 = open("/content/drive/MyDrive/Colab Notebooks/SentenceBERT/result01.txt", 'w')

embedder = SentenceTransformer("jhgan/ko-sbert-sts")

# Corpus sentences:
corpus = rfp01
corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)

# Query sentences:
queries = rfp02


#------------------------ rfp 같을때 시작
# Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity
top_k = 7
c1 = 0
q = 1

for query in queries:
  query_embedding = embedder.encode(query, convert_to_tensor=True)
  cos_scores = util.pytorch_cos_sim(query_embedding, corpus_embeddings)[0]
  cos_scores = cos_scores.cpu()

  # np.argpartition, to only partially sort the top_k results
  top_results = np.argpartition(-cos_scores, range(top_k))[0:top_k]   # [1:top_k] 같은거 빼고~

  print("\n\n======================\n")
  print("\n\n======================\n", file = f04)
  print("Query(%i):" %(q), query + "\n")
  print("Query(%i):" %(q), query + "\n", file = f04)

  for idx in top_results[0:top_k]:
    if q != idx+1:
      print("Corpus(%i): " %(idx+1) + corpus[idx].strip(), "(Score: %.4f)" % (cos_scores[idx]))
      print("Corpus(%i): " %(idx+1) + corpus[idx].strip(), "(Score: %.4f)" % (cos_scores[idx]), file = f04)
      if c1 == 0:
        print("%.4f, %i, %i" %(cos_scores[idx], q, idx), file = f03)
        # print("%.4f, %i, %i" %(cos_scores[idx], q, idx), file = f04)
        c1 += 1

  c1 = 0
  q += 1
#------------------------ rfp 같을때 끝

#------------------------ rfp 다를때 시작
# # Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity
# top_k = 3
# c1 = 0
# q = 1

# for query in queries:
#   query_embedding = embedder.encode(query, convert_to_tensor=True)
#   cos_scores = util.pytorch_cos_sim(query_embedding, corpus_embeddings)[0]
#   cos_scores = cos_scores.cpu()

#   # np.argpartition, to only partially sort the top_k results
#   top_results = np.argpartition(-cos_scores, range(top_k))[0:top_k]

#   print("\n\n======================\n")
#   print("\n\n======================\n", file = f04)
#   print("Query(%i):" %(q), query + "\n")
#   print("Query(%i):" %(q), query + "\n", file = f04)

#   for idx in top_results[0:top_k]:
#     print("Corpus(%i): " %(idx+1) + corpus[idx].strip(), "(Score: %.4f)" % (cos_scores[idx]))
#     print("Corpus(%i): " %(idx+1) + corpus[idx].strip(), "(Score: %.4f)" % (cos_scores[idx]), file = f04)
#     if c1 == 0:
#       print("%.4f, %i, %i" %(cos_scores[idx], q, idx), file = f03)
#       # print("%.4f, %i, %i" %(cos_scores[idx], q, idx), file = f04)
#       c1 += 1

#   c1 = 0
#   q += 1
#------------------------ rfp 다를때 끝

f03.close()
f04.close()

from re import split
import matplotlib.pyplot as plt    #맷플롯립의 pyplot 모듈
import numpy as np

imsi1 = []
a1 = []
b1 = []
c1 = []

f04 = open("/content/drive/MyDrive/Colab Notebooks/SentenceBERT/graph01.txt","r")
content1 = f04.readlines()
content1 = list(map(lambda s: s.strip(), content1))

i = 0
for line in content1:
  imsi1 = content1[i].split(',')
  a1.append(float(imsi1[0]))
  b1.append(float(imsi1[1]))
  c1.append(float(imsi1[2]))
  i += 1

fig, ax = plt.subplots(figsize=(18,6))
# x축에는 query 순서값, y축에는 sbert score값을 표시한다.
plt.plot(b1, a1, color = 'red', marker = 'o', linestyle = 'solid', label='Sentence BERT')
plt.axhline(0.75, 0.01, 0.99, color='blue', linestyle='--', linewidth=1)
# plt.hlines(0.7, 1.0, 66.0, color='green', linestyle='--', linewidth=1) # solid
plt.xticks(np.arange(1, i+1, 1))
plt.yticks(np.arange(0.3, 1.2, 0.1))
plt.legend()

# 제목을 설정
plt.title('Top similar sentence in corpus') # corpus(rfp01)중에서 query(rfp01)와 가장 유사한 문장

#y축에 레이블
plt.ylabel('Score')
plt.xlabel('No. of Query')
plt.savefig('/content/drive/MyDrive/Colab Notebooks/SentenceBERT/graph_Result.png', dpi = 1200)
plt.show()

f04.close()

time.sleep(1)

print(f"{time.time()-start:.4f} sec")